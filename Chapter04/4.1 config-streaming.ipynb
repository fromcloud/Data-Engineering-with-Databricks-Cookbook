{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2cb9c58f-e466-48b4-81ab-07872b03c918",
   "metadata": {},
   "source": [
    "For running this recipe, we first need to set up incoming streaming data. We will feed data by opening a terminal window in Jupyter labs UI and run the following command that uses the nc (netcat) utility to create a socket connection on port 9999 and listen for incoming data: \n",
    "\n",
    "`nc -lk 9999 `\n",
    "\n",
    "Once the previous command is running, you can start typing any text on the command line. \n",
    "\n",
    "For example, you can enter the following text: \n",
    "\n",
    "Fundamentals of Data Engineering: Plan and Build Robust Data Systems by Joe Reis and Matt Housley. This book provides a concise overview of the data engineering landscape and a framework of best practices to assess and solve data engineering problems. It also helps you choose the best technologies and architectures for your data needs. \n",
    " \n",
    "Designing Data-Intensive Applications: The Big Ideas Behind Reliable, Scalable, and Maintainable Systems** by Martin Kleppmann. This book explains the fundamental principles and trade-offs behind the design of distributed data systems. It covers topics such as replication, partitioning, consistency, fault tolerance, batch and stream processing, and data model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "54f45c54-0b95-4fd9-a180-fe3be96ab99d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession  # Spark SQL 작업을 위한 SparkSession 임포트\n",
    "from pyspark.sql.functions import  # Spark SQL 함수들 임포트 explode, split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e7850eab-3759-491d-a70a-7a02977db101",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/02/04 17:37:37 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = (SparkSession.builder  # SparkSession 빌더 패턴 시작\n",
    "           .appName(\"config-streaming\")  # 애플리케이션 이름 설정\n",
    "           .master(\"spark://spark-master:7077\")  # Spark 마스터 URL 설정\n",
    "           .config(\"spark.executor.memory\", \"512m\")  # Spark 설정 옵션\n",
    "           .getOrCreate()  # SparkSession 생성 또는 기존 세션 반환)\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")  # 로그 레벨을 ERROR로 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "228e8f56-4fd9-497a-947f-8ce968889dc7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 생성 DataFrame representing the stream of input lines from connection to localhost:9999\n",
    "lines = (spark.readStream  # 스트리밍 데이터 읽기\n",
    "         .format(\"socket\")\n",
    "         .option(\"host\", \"localhost\")\n",
    "         .option(\"port\", 9999)\n",
    "         .load(  # 파일 로드))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a0e26449-e04a-4cf5-a42f-5594943bd3fa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Split the lines into words\n",
    "words = lines.select(  # 컬럼 선택\n",
    "   explode(  # 배열을 개별 행으로 분해split(lines.value, \" \")).alias(\"word\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f8d3d68c-c3b0-4a8a-91b0-32366f2199fc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Generate running word count\n",
    "wordCounts = words.groupBy(  # 그룹화\"word\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5c447d82-f3f3-4b78-98b9-cb8e1e79512c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 0\n",
      "-------------------------------------------\n",
      "+----+-----+\n",
      "|word|count|\n",
      "+----+-----+\n",
      "+----+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 1\n",
      "-------------------------------------------\n",
      "+------------+-----+\n",
      "|        word|count|\n",
      "+------------+-----+\n",
      "|        Data|    2|\n",
      "|    overview|    1|\n",
      "|Fundamentals|    1|\n",
      "|      stream|    1|\n",
      "|          by|    2|\n",
      "|       solve|    1|\n",
      "|         you|    1|\n",
      "|   landscape|    1|\n",
      "|    systems.|    1|\n",
      "|replication,|    1|\n",
      "|         for|    1|\n",
      "|         Joe|    1|\n",
      "|  tolerance,|    1|\n",
      "|    provides|    1|\n",
      "|        Reis|    1|\n",
      "|      topics|    1|\n",
      "|   practices|    1|\n",
      "|       model|    1|\n",
      "|     concise|    1|\n",
      "| distributed|    1|\n",
      "+------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 2\n",
      "-------------------------------------------\n",
      "+------------+-----+\n",
      "|        word|count|\n",
      "+------------+-----+\n",
      "|   Dynamical|    1|\n",
      "|        Data|    2|\n",
      "|     complex|    1|\n",
      "|    overview|    1|\n",
      "|     Science|    1|\n",
      "|Fundamentals|    1|\n",
      "|      stream|    1|\n",
      "|      Nathan|    1|\n",
      "|          by|    3|\n",
      "|       solve|    2|\n",
      "|         you|    2|\n",
      "|   landscape|    1|\n",
      "|          L.|    1|\n",
      "|    systems.|    1|\n",
      "|       apply|    1|\n",
      "|replication,|    1|\n",
      "|         for|    1|\n",
      "|         Joe|    1|\n",
      "|         how|    1|\n",
      "|  reduction,|    1|\n",
      "+------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    " # Start running the query that prints the running counts to the console\n",
    "query = (wordCounts.writeStream  # 스트리밍 데이터 쓰기\n",
    "         .outputMode(  # 스트리밍 출력 모드 설정\"complete\")\n",
    "         .format(\"console\")\n",
    "         .start()  # 스트리밍 시작)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1330621-ab2a-4e08-b5c2-41312d472fd5",
   "metadata": {},
   "source": [
    "Open the terminal and add more data to the netcat listener. See the following example text: \n",
    "\n",
    "__Data-Driven Science and Engineering: Machine Learning, Dynamical Systems, and Control by Steven L. Brunton and J. Nathan Kutz13. This book teaches you how to apply machine learning and data analytics techniques to solve complex engineering and scientific problems. It covers topics such as dimensionality reduction, sparse sensing, system identification, and control design.__\n",
    "\n",
    "A new batch for the stream query is triggered and the output is updated as shown: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bbdd7547-1607-4aa8-9e44-9e82f7356ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "query.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3fb39cc3-e38d-4bcb-96d3-7493e83f8c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()  # Spark 세션 종료 - 리소스 정리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6410a564-9653-4ece-aeb4-e56df9bd881d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}