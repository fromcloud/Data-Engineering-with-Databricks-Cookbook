{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3d05d7a1-fe97-491a-a177-c1886a5f8baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession  # Spark SQL 작업을 위한 SparkSession 임포트 \n",
    "from pyspark.sql.functions import  # Spark SQL 함수들 임포트 rand, col, when, broadcast, concat, lit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6bfbe105-2fd1-43f1-95e9-525d85226a13",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/02/21 13:50:51 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "# 생성 a new SparkSession\n",
    "spark = (SparkSession\n",
    "         .builder\n",
    "         .appName(\"avoid-data-skew\")  # 애플리케이션 이름 설정\n",
    "         .master(\"spark://spark-master:7077\")  # Spark 마스터 URL 설정\n",
    "         .config(\"spark.executor.memory\", \"512m\")  # Spark 설정 옵션\n",
    "         .getOrCreate()  # SparkSession 생성 또는 기존 세션 반환)\n",
    "\n",
    "# Set log level to ERROR\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")  # 로그 레벨을 ERROR로 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "32a36dec-c284-4e7f-9498-74e9d35863e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to measure the execution time of a query\n",
    "import time\n",
    "\n",
    "def measure_time(query):\n",
    "    start = time.time()\n",
    "    query.collect() # Force the query execution by calling an action\n",
    "    end = time.time()\n",
    "    print(f\"Execution time: {end - start} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2ae2e1f7-45c3-486a-970e-0727eb303197",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 생성 some sample data frames\n",
    "# A large data frame with 10 million rows and two columns: id and value\n",
    "large_df = spark.range(0, 10000000).withColumn(  # 새 컬럼 추가 또는 기존 컬럼 수정\"value\", rand(seed=42))\n",
    "\n",
    "# A skewed data frame with 1 million rows and two columns: id and value\n",
    "skewed_df = spark.range(0, 1000000).withColumn(  # 새 컬럼 추가 또는 기존 컬럼 수정\"value\", rand(seed=42)).withColumn(  # 새 컬럼 추가 또는 기존 컬럼 수정\"id\", when(col(\"id\")%4 == 0, 0).otherwise(col(\"id\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2e62337f-37c7-4575-9521-041301873c0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 0:>                                                          (0 + 2) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of partitions: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partition sizes: [1998962, 2000902, 1999898, 2000588, 1999650]\n",
      "Number of partitions: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 5:===================================>                       (3 + 2) / 5]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partition sizes: [400054, 150144, 149846, 149903, 150053]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "large_df_repartitioned = large_df.repartition(  # 파티션 재분배5, \"id\")\n",
    "num_partitions = large_df_repartitioned.rdd.getNumPartitions()\n",
    "print(f\"Number of partitions: {num_partitions}\")\n",
    "\n",
    "partition_sizes = large_df_repartitioned.rdd.glom().map(len).collect()\n",
    "print(f\"Partition sizes: {partition_sizes}\")\n",
    "\n",
    "skewed_df_repartitioned = skewed_df.repartition(  # 파티션 재분배5, \"id\")\n",
    "num_partitions = skewed_df_repartitioned.rdd.getNumPartitions()\n",
    "print(f\"Number of partitions: {num_partitions}\")\n",
    "\n",
    "partition_sizes = skewed_df_repartitioned.rdd.glom().map(len).collect()\n",
    "print(f\"Partition sizes: {partition_sizes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8741aa2b-a682-4878-88a9-addbcc5605bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(  # Spark 설정 변경\"spark.sql.adaptive.enabled\", \"false\")\n",
    "spark.conf.set(  # Spark 설정 변경\"spark.sql.autoBroadcastJoinThreshold\", -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6cfa7b10-4c39-4e0d-91fc-5a166660acce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time: 30.910954236984253 seconds\n"
     ]
    }
   ],
   "source": [
    "# Join the non-skewed DataFrames using the default join strategy (sort-merge join)\n",
    "inner_join_df = large_df_repartitioned.join(  # 데이터프레임 조인skewed_df_repartitioned, \"id\")\n",
    "measure_time(inner  # 내부 조인_join_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ab8cf88f-2f78-4311-b203-cf3a3ca45f1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(5) Project [id#0L, value#2, value#7]\n",
      "+- *(5) SortMergeJoin [id#0L], [id#10L], Inner\n",
      "   :- *(2) Sort [id#0L ASC NULLS FIRST], false, 0\n",
      "   :  +- Exchange hashpartitioning(id#0L, 200), REPARTITION_BY_NUM, [plan_id=97]\n",
      "   :     +- *(1) Project [id#0L, rand(42) AS value#2]\n",
      "   :        +- *(1) Range (0, 10000000, step=1, splits=2)\n",
      "   +- *(4) Sort [id#10L ASC NULLS FIRST], false, 0\n",
      "      +- Exchange hashpartitioning(id#10L, 200), REPARTITION_BY_NUM, [plan_id=103]\n",
      "         +- *(3) Project [CASE WHEN ((id#5L % 4) = 0) THEN 0 ELSE id#5L END AS id#10L, value#7]\n",
      "            +- *(3) Project [id#5L, rand(42) AS value#7]\n",
      "               +- *(3) Range (0, 1000000, step=1, splits=2)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "inner  # 내부 조인_join_df.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cec6e249-0376-4518-897e-8dbf94e7357b",
   "metadata": {},
   "source": [
    "### Isolate skewed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "89bad180-c8e2-474f-8279-282fe3611975",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time: 14.912306070327759 seconds\n"
     ]
    }
   ],
   "source": [
    "# Identify the skewed value in the invoice_id column\n",
    "skewed_value = 0\n",
    "\n",
    "# 필터링 out the rows with the skewed value from both DataFrames\n",
    "large_skewed_df = large_df_repartitioned.filter(  # 데이터 필터링large_df_repartitioned.id == skewed_value)\n",
    "small_skewed_df = skewed_df_repartitioned.filter(  # 데이터 필터링skewed_df_repartitioned.id == skewed_value)\n",
    "\n",
    "# 필터링 out the rows without the skewed value from both DataFrames\n",
    "large_non_skewed_df = large_df_repartitioned.filter(  # 데이터 필터링large_df_repartitioned.id != skewed_value)\n",
    "small_non_skewed_df = skewed_df_repartitioned.filter(  # 데이터 필터링skewed_df_repartitioned.id != skewed_value)\n",
    "\n",
    "# Join the non-skewed DataFrames using the default join strategy (sort-merge join)\n",
    "non_skewed_join_df = large_non_skewed_df.join(  # 데이터프레임 조인small_non_skewed_df, \"id\")\n",
    "\n",
    "# Join the skewed DataFrames using a broadcast hash join\n",
    "skewed_join_df = large_skewed_df.join(  # 데이터프레임 조인broadcast(small_skewed_df), \"id\")\n",
    "\n",
    "# Union the results from both joins\n",
    "final_join_df = non_skewed_join_df.union(  # 데이터프레임 합치기skewed_join_df)\n",
    "\n",
    "measure_time(final_join_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9d0d6cab-4465-460e-99d0-efcf7d3e7495",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "Union\n",
      ":- *(5) Project [id#0L, value#2, value#7]\n",
      ":  +- *(5) SortMergeJoin [id#0L], [id#10L], Inner\n",
      ":     :- *(2) Sort [id#0L ASC NULLS FIRST], false, 0\n",
      ":     :  +- Exchange hashpartitioning(id#0L, 200), REPARTITION_BY_NUM, [plan_id=228]\n",
      ":     :     +- *(1) Filter NOT (id#0L = 0)\n",
      ":     :        +- *(1) Project [id#0L, rand(42) AS value#2]\n",
      ":     :           +- *(1) Range (0, 10000000, step=1, splits=2)\n",
      ":     +- *(4) Sort [id#10L ASC NULLS FIRST], false, 0\n",
      ":        +- Exchange hashpartitioning(id#10L, 200), REPARTITION_BY_NUM, [plan_id=234]\n",
      ":           +- *(3) Project [CASE WHEN ((id#5L % 4) = 0) THEN 0 ELSE id#5L END AS id#10L, value#7]\n",
      ":              +- *(3) Filter NOT CASE WHEN ((id#5L % 4) = 0) THEN true ELSE (id#5L = 0) END\n",
      ":                 +- *(3) Project [id#5L, rand(42) AS value#7]\n",
      ":                    +- *(3) Range (0, 1000000, step=1, splits=2)\n",
      "+- *(8) Project [id#25L, value#2, value#7]\n",
      "   +- *(8) BroadcastHashJoin [id#25L], [id#10L], Inner, BuildRight, false\n",
      "      :- Exchange hashpartitioning(id#25L, 5), REPARTITION_BY_NUM, [plan_id=243]\n",
      "      :  +- *(6) Filter (id#25L = 0)\n",
      "      :     +- *(6) Project [id#25L, rand(42) AS value#2]\n",
      "      :        +- *(6) Range (0, 10000000, step=1, splits=2)\n",
      "      +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, bigint, false]),false), [plan_id=247]\n",
      "         +- Exchange hashpartitioning(id#10L, 5), REPARTITION_BY_NUM, [plan_id=246]\n",
      "            +- *(7) Project [CASE WHEN ((id#26L % 4) = 0) THEN 0 ELSE id#26L END AS id#10L, value#7]\n",
      "               +- *(7) Filter CASE WHEN ((id#26L % 4) = 0) THEN true ELSE (id#26L = 0) END\n",
      "                  +- *(7) Project [id#26L, rand(42) AS value#7]\n",
      "                     +- *(7) Range (0, 1000000, step=1, splits=2)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "final_join_df.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e21e25c-d231-4177-8aa4-cf2e01222d4e",
   "metadata": {},
   "source": [
    "### Broadcast hash join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8cdbc81e-7451-4b42-a49f-c27de2963304",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time: 8.040036916732788 seconds\n"
     ]
    }
   ],
   "source": [
    "smaller_df = skewed_df_repartitioned\n",
    "\n",
    "# Use the broadcast function to mark the smaller DataFrame for broadcasting\n",
    "from pyspark.sql.functions import  # Spark SQL 함수들 임포트 broadcast\n",
    "broadcast_df = broadcast(smaller_df)\n",
    "\n",
    "# Join the two DataFrames using the broadcast function as an argument\n",
    "\n",
    "broadcast_join_df = large_df_repartitioned.join(  # 데이터프레임 조인broadcast_df, \"id\")\n",
    "\n",
    "measure_time(broadcast_join_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6e35d6d7-1484-4b61-a60f-4db911776dee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(3) Project [id#0L, value#2, value#7]\n",
      "+- *(3) BroadcastHashJoin [id#0L], [id#10L], Inner, BuildRight, false\n",
      "   :- Exchange hashpartitioning(id#0L, 5), REPARTITION_BY_NUM, [plan_id=329]\n",
      "   :  +- *(1) Project [id#0L, rand(42) AS value#2]\n",
      "   :     +- *(1) Range (0, 10000000, step=1, splits=2)\n",
      "   +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, bigint, false]),false), [plan_id=333]\n",
      "      +- Exchange hashpartitioning(id#10L, 5), REPARTITION_BY_NUM, [plan_id=332]\n",
      "         +- *(2) Project [CASE WHEN ((id#5L % 4) = 0) THEN 0 ELSE id#5L END AS id#10L, value#7]\n",
      "            +- *(2) Project [id#5L, rand(42) AS value#7]\n",
      "               +- *(2) Range (0, 1000000, step=1, splits=2)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "broadcast_join_df.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c3ab080c-9db3-499a-97a7-010327cd2bfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 22:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------------+-------------------+\n",
      "| id|            value|              value|\n",
      "+---+-----------------+-------------------+\n",
      "|  0|0.619189370225301|0.14968092420202395|\n",
      "|  0|0.619189370225301| 0.8421636914011397|\n",
      "|  0|0.619189370225301| 0.5516706309356983|\n",
      "|  0|0.619189370225301| 0.5908099559659594|\n",
      "|  0|0.619189370225301|0.08615681416391996|\n",
      "|  0|0.619189370225301|0.15134594584450656|\n",
      "|  0|0.619189370225301|  0.657124398921156|\n",
      "|  0|0.619189370225301| 0.3063672834878989|\n",
      "|  0|0.619189370225301|0.11190167037821486|\n",
      "|  0|0.619189370225301|0.07747379719322578|\n",
      "+---+-----------------+-------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# 출력 some rows of the result\n",
    "broadcast_join_df.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "066a49d7-39f7-419e-ab48-57ae28e405e1",
   "metadata": {},
   "source": [
    "### Key salting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "057f9ee3-3f75-40d8-a2e6-8b8d4dfad150",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time: 19.14427161216736 seconds\n"
     ]
    }
   ],
   "source": [
    "# Import random module\n",
    "import random\n",
    "\n",
    "# Identify the skewed value in the id column\n",
    "skewed_value = 0\n",
    "\n",
    "# 생성 a list of salt values to append to the skewed value\n",
    "salt_list = [\"_A\", \"_B\", \"_C\", \"_D\", \"_E\"]\n",
    "\n",
    "# 생성 a new column in both DataFrames that contains the original invoice_id value plus a salt value if it is skewed, or just the original invoice_id value otherwise\n",
    "large_df = (large_df_repartitioned\n",
    "              .withColumn(  # 새 컬럼 추가 또는 기존 컬럼 수정\"salted_id\", when(large_df_repartitioned.id == skewed_value, concat(large_df_repartitioned.id, lit(random.choice(salt_list))))\n",
    "                          .otherwise(  # 기본값 설정large_df_repartitioned.id)))\n",
    "skewed_df = (skewed_df_repartitioned\n",
    "             .withColumn(  # 새 컬럼 추가 또는 기존 컬럼 수정\"salted_id\", when(skewed_df_repartitioned.id == skewed_value, concat(skewed_df_repartitioned.id, lit(random.choice(salt_list))))\n",
    "                         .otherwise(  # 기본값 설정skewed_df_repartitioned.id)))\n",
    "\n",
    "# Join the two DataFrames on the new column using the default join strategy (sort-merge join)\n",
    "salted_join_df = large_df.join(  # 데이터프레임 조인skewed_df, \"salted_id\")\n",
    "\n",
    "# Drop the new column and keep only the original invoice_id column\n",
    "final_join_df = salted_join_df.drop(  # 컬럼 삭제\"salted_id\")\n",
    "\n",
    "measure_time(final_join_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f5f364d0-b1e4-44d1-91fb-cd72dbcd9509",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 31:==============================================>           (4 + 1) / 5]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------------------+------+-------------------+\n",
      "|    id|              value|    id|              value|\n",
      "+------+-------------------+------+-------------------+\n",
      "|100010|0.35369292037242683|100010|0.35369292037242683|\n",
      "|100227| 0.7173545475305475|100227| 0.7173545475305475|\n",
      "|100263| 0.6839437246645035|100263| 0.6839437246645035|\n",
      "|100553| 0.9213033942226746|100553| 0.9213033942226746|\n",
      "|100735| 0.5717367801064485|100735| 0.5717367801064485|\n",
      "|101021|0.36029429102236565|101021|0.36029429102236565|\n",
      "|101122| 0.7321914376505848|101122| 0.7321914376505848|\n",
      "|101205| 0.5124474456736382|101205| 0.5124474456736382|\n",
      "|101261| 0.6677580792714339|101261| 0.6677580792714339|\n",
      "|102113| 0.7728354101123371|102113| 0.7728354101123371|\n",
      "+------+-------------------+------+-------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# 출력 some rows of the result\n",
    "final_join_df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e74d189c-3b16-4b11-8d4d-af1534553f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()  # Spark 세션 종료 - 리소스 정리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "301a776a-c753-496e-a9f3-b85cd2d9a0d3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}