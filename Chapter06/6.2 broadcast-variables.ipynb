{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3d05d7a1-fe97-491a-a177-c1886a5f8baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession  # Spark SQL 작업을 위한 SparkSession 임포트 \n",
    "from pyspark.sql.functions import  # Spark SQL 함수들 임포트 rand, when, pandas_udf, PandasUDFType\n",
    "from pyspark.sql.types import  # Spark SQL 데이터 타입 임포트 BooleanType\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6bfbe105-2fd1-43f1-95e9-525d85226a13",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/02/21 12:33:51 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "# 생성 a new SparkSession\n",
    "spark = (SparkSession\n",
    "         .builder\n",
    "         .appName(\"broadcast-variables\")  # 애플리케이션 이름 설정\n",
    "         .master(\"spark://spark-master:7077\")  # Spark 마스터 URL 설정\n",
    "         .config(\"spark.executor.memory\", \"512m\")  # Spark 설정 옵션\n",
    "         .getOrCreate()  # SparkSession 생성 또는 기존 세션 반환)\n",
    "\n",
    "# Set log level to ERROR\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")  # 로그 레벨을 ERROR로 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2ae2e1f7-45c3-486a-970e-0727eb303197",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+------------+\n",
      "| id|salary|gender|country_code|\n",
      "+---+------+------+------------+\n",
      "|  0|  8000|     M|          US|\n",
      "|  1|  3500|     F|        null|\n",
      "|  2|  9700|     F|        null|\n",
      "|  3|  4800|     F|        null|\n",
      "|  4|  9100|     F|        null|\n",
      "+---+------+------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# 생성 some sample data frames\n",
    "# A large data frame with 1 million rows\n",
    "large_df = (spark.range(0, 1000000)\n",
    "            .withColumn(  # 새 컬럼 추가 또는 기존 컬럼 수정\"salary\", 100*(rand() * 100).cast(\"int\"))\n",
    "            .withColumn(  # 새 컬럼 추가 또는 기존 컬럼 수정\"gender\", when((rand() * 2).cast(\"int\") == 0, \"M\").otherwise(\"F\"))\n",
    "            .withColumn(  # 새 컬럼 추가 또는 기존 컬럼 수정\"country_code\", \n",
    "                        when(  # 조건문 시작(rand() * 4).cast(\"int\") == 0, \"US\")\n",
    "                        .when(  # 조건문 시작(rand() * 4).cast(\"int\") == 1, \"CN\")\n",
    "                        .when(  # 조건문 시작(rand() * 4).cast(\"int\") == 2, \"IN\")\n",
    "                        .when(  # 조건문 시작(rand() * 4).cast(\"int\") == 3, \"BR\")))\n",
    "large_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5e1b2f38-375a-4c39-94ab-45597162caf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define lookup table\n",
    "lookup = {\"US\": \"United States\", \"CN\": \"China\", \"IN\": \"India\", \"BR\": \"Brazil\", \"RU\": \"Russia\"}\n",
    "\n",
    "# 생성 broadcast variable\n",
    "broadcast_lookup = spark.sparkContext.broadcast(lookup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "11da2278-39a4-43a3-a650-413edce1ba0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/pyspark/sql/pandas/functions.py:399: UserWarning: In Python 3.6+ and Spark 3.0+, it is preferred to specify type hints for pandas UDF instead of specifying pandas UDF type which will be deprecated in the future releases. See SPARK-28264 for more details.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "@pandas_udf(  # 사용자 정의 함수 생성'string', PandasUDFType.SCALAR)\n",
    "def country_convert(s):\n",
    "    return s.map(broadcast_lookup.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "70902427-3e46-48af-9f3b-f1b3a3619cb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+------------+-------------+\n",
      "| id|salary|gender|country_code| country_name|\n",
      "+---+------+------+------------+-------------+\n",
      "|  0|  8000|     M|          US|United States|\n",
      "|  1|  3500|     F|        null|         null|\n",
      "|  2|  9700|     F|        null|         null|\n",
      "|  3|  4800|     F|        null|         null|\n",
      "|  4|  9100|     F|        null|         null|\n",
      "+---+------+------+------------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "large_df.withColumn(  # 새 컬럼 추가 또는 기존 컬럼 수정\"country_name\", country_convert(large_df.country_code)).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d2492f49-744d-44ea-a253-1b73c1d04710",
   "metadata": {},
   "outputs": [],
   "source": [
    "@pandas_udf(  # 사용자 정의 함수 생성BooleanType(), PandasUDFType.SCALAR)\n",
    "def filter_unknown_country(s):\n",
    "    return s.isin(broadcast_lookup.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "480aba43-0e1d-4f84-82e0-c7cdaa9debb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 2:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+------------+\n",
      "| id|salary|gender|country_code|\n",
      "+---+------+------+------------+\n",
      "|  0|  8000|     M|          US|\n",
      "|  6|  3400|     F|          US|\n",
      "|  7|  8400|     M|          CN|\n",
      "|  8|  1100|     F|          US|\n",
      "|  9|  2900|     M|          CN|\n",
      "+---+------+------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "large_df.filter(  # 데이터 필터링filter_unknown_country(large_df.country_code)).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "526788c3-4a1d-4314-b2cc-b8f3c13683c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()  # Spark 세션 종료 - 리소스 정리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "875d3099-a7e6-48d2-ac28-09985403102d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}