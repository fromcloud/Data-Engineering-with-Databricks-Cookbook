{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6020138a-0dc8-4b7e-bad8-9cf2ef7133aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import modules\n",
    "from pyspark.sql import SparkSession  # Spark SQL 작업을 위한 SparkSession 임포트\n",
    "from pyspark.sql.functions import  # Spark SQL 함수들 임포트 broadcast, col, rand, skewness,lit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2b24dc6e-9c01-4a8b-9b6e-0d71b953bd82",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = (SparkSession.builder  # SparkSession 빌더 패턴 시작\n",
    "         .appName(\"optimize-join-strategies\")  # 애플리케이션 이름 설정\n",
    "         .master(\"spark://spark-master:7077\")  # Spark 마스터 URL 설정\n",
    "         .config(\"spark.executor.memory\", \"512m\")  # Spark 설정 옵션\n",
    "         .getOrCreate()  # SparkSession 생성 또는 기존 세션 반환)\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")  # 로그 레벨을 ERROR로 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "20292d5d-76f1-4974-8c2a-a3c7f891762a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 생성 some sample data frames\n",
    "# A large data frame with 10 million rows and two columns: id and value\n",
    "large_df = spark.range(0, 1000000).withColumn(  # 새 컬럼 추가 또는 기존 컬럼 수정\"value\", rand(seed=42))\n",
    "\n",
    "# A small data frame with 10000 rows and two columns: id and name\n",
    "small_df = spark.range(0, 10000).withColumn(  # 새 컬럼 추가 또는 기존 컬럼 수정\"name\", col(\"id\").cast(\"string\"))\n",
    "\n",
    "# A skewed data frame with 10 million rows and two columns: id and value\n",
    "# The id column has a Zipf distribution with a skewness of 4.7\n",
    "skewed_df = spark.range(0, 1000000).withColumn(  # 새 컬럼 추가 또는 기존 컬럼 수정\"value\", rand(seed=42)).withColumn(  # 새 컬럼 추가 또는 기존 컬럼 수정\"id\", col(\"id\") ** 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "be71d3cf-5eb5-4fdc-ac15-05139c979aa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to measure the execution time of a query\n",
    "import time\n",
    "\n",
    "def measure_time(query):\n",
    "    start = time.time()\n",
    "    query.collect() # Force the query execution by calling an action\n",
    "    end = time.time()\n",
    "    print(f\"Execution time: {end - start} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65991527-3618-47df-89c9-16980642499b",
   "metadata": {},
   "source": [
    "## Choosing the right join type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "972f6ebe-8ad6-418b-bf6d-3352be4cea1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time: 1.0291528701782227 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time: 25.628353357315063 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time: 5.767467021942139 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time: 18.269603490829468 seconds\n",
      "Execution time: 0.5185227394104004 seconds\n",
      "Execution time: 6.6788036823272705 seconds\n"
     ]
    }
   ],
   "source": [
    "# Join large_df and small_df using an inner  # 내부 조인 join on id column\n",
    "measure_time(large_df.join(  # 데이터프레임 조인small_df, \"id\"))\n",
    "\n",
    "# Join large_df and small_df using a left  # 왼쪽 조인 outer join on id column\n",
    "measure_time(large_df.join(  # 데이터프레임 조인small_df, \"id\", \"left\"))\n",
    "\n",
    "# Join large_df and small_df using a right  # 오른쪽 조인 outer join on id column\n",
    "measure_time(large_df.join(  # 데이터프레임 조인small_df, \"id\", \"right\"))\n",
    "\n",
    "# Join large_df and small_df using a full outer  # 외부 조인 join on id column\n",
    "measure_time(large_df.join(  # 데이터프레임 조인small_df, \"id\", \"full\"))\n",
    "\n",
    "# Join large_df and small_df using a left  # 왼쪽 조인 semi join on id column\n",
    "measure_time(large_df.join(  # 데이터프레임 조인small_df, \"id\", \"left_semi\"))\n",
    "\n",
    "# Join large_df and small_df using a left  # 왼쪽 조인 anti join on id column\n",
    "measure_time(large_df.join(  # 데이터프레임 조인small_df, \"id\", \"left_anti\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7e2e68d-f1b3-48ac-a60c-545eef5af335",
   "metadata": {},
   "source": [
    "## Broadcasting small tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e5502612-ab6c-4b4c-a90b-aa478d385639",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time: 3.1970551013946533 seconds\n",
      "Execution time: 0.20557928085327148 seconds\n"
     ]
    }
   ],
   "source": [
    "# Join large_df and small_df using an inner  # 내부 조인 join with broadcast hash join hint\n",
    "spark.conf.set(  # Spark 설정 변경\"spark.sql.adaptive.enabled\", \"false\")\n",
    "spark.conf.set(  # Spark 설정 변경\"spark.sql.autoBroadcastJoinThreshold\", -1)\n",
    "\n",
    "# Join large_df and small_df using an inner  # 내부 조인 join without broadcasting\n",
    "measure_time(large_df.join(  # 데이터프레임 조인small_df, \"id\"))\n",
    "\n",
    "# Join large_df and small_df using an inner  # 내부 조인 join with broadcasting\n",
    "measure_time(large_df.join(  # 데이터프레임 조인broadcast(small_df), \"id\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94f15cf6-3a64-40c5-b099-36a7796fb532",
   "metadata": {},
   "source": [
    "## Using Join Hints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "287e85d6-65b0-4436-8658-11b370f3318a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time: 1.8980967998504639 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time: 2.253967046737671 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time: 761.0224421024323 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 45:=========================================>            (155 + 6) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time: 2.357747793197632 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Join large_df and small_df using an inner  # 내부 조인 join with broadcast hash join hint\n",
    "inner_join_broadcast_hint = large_df.hint(\"broadcast\").join(  # 데이터프레임 조인small_df, \"id\")\n",
    "measure_time(inner  # 내부 조인_join_broadcast_hint)\n",
    "\n",
    "# Join large_df and small_df using an inner  # 내부 조인 join with shuffle hash join hint\n",
    "inner_join_shuffle_hash_hint = large_df.hint(\"shuffle_hash\").join(  # 데이터프레임 조인small_df, \"id\")\n",
    "measure_time(inner  # 내부 조인_join_shuffle_hash_hint)\n",
    "\n",
    "# Join large_df and small_df using an inner  # 내부 조인 join with shuffle replicate nested loop join hint\n",
    "inner_join_shuffle_replicate_nl_hint = large_df.hint(\"shuffle_replicate_nl\").join(  # 데이터프레임 조인small_df, \"id\")\n",
    "measure_time(inner  # 내부 조인_join_shuffle_replicate_nl_hint)\n",
    "\n",
    "# Join large_df and small_df using an inner  # 내부 조인 join with sort merge join hint\n",
    "inner_join_merge_hint = large_df.hint(\"merge\").join(  # 데이터프레임 조인small_df, \"id\")\n",
    "measure_time(inner  # 내부 조인_join_merge_hint)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9df7de7a-f5e9-4c2e-b5d1-34b9be9030e5",
   "metadata": {},
   "source": [
    "## Enable Adaptive Query Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0c08cf59-466d-4071-8ef1-93827484fb2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time: 8.188302278518677 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 58:=========>                                                (1 + 5) / 6]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time: 2.7499380111694336 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Join large_df and skewed_df using an inner  # 내부 조인 join without AQE\n",
    "spark.conf.set(  # Spark 설정 변경\"spark.sql.adaptive.enabled\", \"false\")\n",
    "inner_join_no_aqe = large_df.join(  # 데이터프레임 조인skewed_df, \"id\")\n",
    "measure_time(inner  # 내부 조인_join_no_aqe)\n",
    "\n",
    "# Join large_df and skewed_df using an inner  # 내부 조인 join with AQE\n",
    "spark.conf.set(  # Spark 설정 변경\"spark.sql.adaptive.enabled\", \"true\")\n",
    "inner_join_aqe = large_df.join(  # 데이터프레임 조인skewed_df, \"id\")\n",
    "measure_time(inner  # 내부 조인_join_aqe)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49507e54-b025-4b63-b789-dac94661007e",
   "metadata": {},
   "source": [
    "## Handling skewed data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa9aef30-f533-4e37-abcc-bff1d8583ee9",
   "metadata": {},
   "source": [
    "### Salting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "90e9821e-34fa-4236-8cd7-93ca6db73cc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 64:=========>                                                (1 + 5) / 6]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time: 6.502509117126465 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Join large_df and skewed_df using an inner  # 내부 조인 join with salting\n",
    "# Add a salt column to the skewed_df with 10 random values\n",
    "skewed_df_with_salt = skewed_df.withColumn(  # 새 컬럼 추가 또는 기존 컬럼 수정\"salt\", (rand(seed=42) * 10).cast(\"int\"))\n",
    "\n",
    "# Join large_df and skewed_df_with_salt on id and salt columns\n",
    "salted_join = large_df.withColumn(  # 새 컬럼 추가 또는 기존 컬럼 수정\"salt\", lit(0)).join(skewed_df_with_salt, [\"id\", \"salt\"])\n",
    "\n",
    "# Remove the salt column and self-join on id column\n",
    "salted_join_no_salt = salted_join.drop(\"salt\").join(skewed_df.select(  # 컬럼 선택\"id\"), \"id\")\n",
    "measure_time(salted_join_no_salt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daf419d4-f6c3-4f07-b60e-93fb0ca3e9af",
   "metadata": {},
   "source": [
    "### Repartitioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "82f2114e-920d-4770-9b60-71d0d5bfe25a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 72:====================================================>(987 + 6) / 1000]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time: 10.696999549865723 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Join large_df and skewed_df using an inner  # 내부 조인 join with repartitioning\n",
    "# Repartition the skewed_df into 1000 partitions\n",
    "skewed_df_repartitioned = skewed_df.repartition(  # 파티션 재분배1000, \"id\")\n",
    "\n",
    "# Join large_df and skewed_df_repartitioned on id column\n",
    "repartitioned_join = large_df.join(  # 데이터프레임 조인skewed_df_repartitioned, \"id\")\n",
    "measure_time(repartitioned_join)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "39884056-8d4d-4616-bd58-91948a3ad93a",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()  # Spark 세션 종료 - 리소스 정리"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}